{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from batchloader import TensorBatchDataset, BatchDataLoader\n",
    "from preprocess import PreprocessDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 103)\n",
      "      feature_cat_0  feature_cat_1  feature_cont_0  feature_cont_1  \\\n",
      "0               1.0            1.0       -1.059951        1.462076   \n",
      "1               1.0            0.0       -0.895100       -0.028954   \n",
      "2               1.0            1.0        2.064241        0.415540   \n",
      "3               0.0            0.0       -0.121514       -0.314254   \n",
      "4               1.0            0.0        1.369914        0.235226   \n",
      "5               0.0            0.0       -1.264873        0.010237   \n",
      "6               0.0            0.0       -1.405209       -0.252663   \n",
      "7               0.0            0.0        0.752061        0.054361   \n",
      "8               1.0            1.0       -1.559383        1.016467   \n",
      "9               1.0            1.0        0.141752       -0.735109   \n",
      "10              0.0            1.0        1.396514        0.011593   \n",
      "11              0.0            0.0        0.596260        0.142328   \n",
      "12              1.0            1.0        1.279563       -0.730809   \n",
      "13              0.0            1.0       -1.986789       -0.182392   \n",
      "14              0.0            0.0       -0.550619        0.173903   \n",
      "15              1.0            1.0       -0.071080       -0.571754   \n",
      "16              0.0            0.0        0.446378        0.106764   \n",
      "17              0.0            1.0        0.038690       -0.599763   \n",
      "18              0.0            0.0       -0.213053        1.717497   \n",
      "19              0.0            1.0       -0.633680        1.269479   \n",
      "20              1.0            1.0       -1.545588       -0.262121   \n",
      "21              1.0            1.0        0.623891        1.112965   \n",
      "22              0.0            0.0        0.547282       -1.750412   \n",
      "23              1.0            1.0        0.357419       -0.018952   \n",
      "24              0.0            1.0        1.058330       -0.730754   \n",
      "25              0.0            0.0       -0.204320       -1.928279   \n",
      "26              1.0            1.0       -0.239481       -0.738614   \n",
      "27              0.0            0.0        1.132802       -0.700730   \n",
      "28              0.0            1.0        1.040554        0.682386   \n",
      "29              1.0            1.0       -0.409355       -0.675098   \n",
      "...             ...            ...             ...             ...   \n",
      "7970            0.0            1.0       -0.750717       -0.059190   \n",
      "7971            0.0            1.0        0.490599        0.749236   \n",
      "7972            1.0            1.0        0.396354        0.245807   \n",
      "7973            0.0            0.0        0.601855        1.101577   \n",
      "7974            1.0            1.0        1.262962        0.960886   \n",
      "7975            1.0            1.0        1.058713        1.193578   \n",
      "7976            1.0            1.0        0.285736        0.110706   \n",
      "7977            0.0            0.0       -0.548862        1.194039   \n",
      "7978            1.0            0.0       -0.145137       -2.030466   \n",
      "7979            1.0            1.0        0.975535       -0.603745   \n",
      "7980            0.0            0.0        0.886528       -0.767413   \n",
      "7981            1.0            0.0       -0.360183       -0.387533   \n",
      "7982            1.0            1.0       -0.078910       -1.578315   \n",
      "7983            1.0            1.0       -0.193747        0.596108   \n",
      "7984            0.0            1.0        0.210294        0.270658   \n",
      "7985            1.0            1.0       -0.390839       -1.237563   \n",
      "7986            0.0            1.0       -0.131253        0.855387   \n",
      "7987            0.0            0.0       -0.311646        0.876069   \n",
      "7988            1.0            0.0       -0.051764       -0.222295   \n",
      "7989            0.0            1.0        0.393628       -2.444055   \n",
      "7990            0.0            0.0       -0.279300       -1.568838   \n",
      "7991            0.0            1.0       -0.350001       -0.210201   \n",
      "7992            0.0            0.0        1.069195       -0.356427   \n",
      "7993            0.0            0.0        0.640573        0.901543   \n",
      "7994            1.0            0.0       -0.057976        0.340099   \n",
      "7995            0.0            0.0        1.556791        0.953532   \n",
      "7996            0.0            1.0        1.820358       -1.463295   \n",
      "7997            1.0            1.0       -1.148540        0.391226   \n",
      "7998            0.0            1.0       -0.455538        0.790680   \n",
      "7999            1.0            0.0       -0.037630        0.460884   \n",
      "\n",
      "      feature_cont_2  feature_cont_3  feature_cont_4  feature_cont_5  \\\n",
      "0           0.498596        1.225902       -0.996361        0.210568   \n",
      "1           0.334408        0.769021        0.652022       -0.266910   \n",
      "2           0.705970       -0.156602        0.336028        2.043391   \n",
      "3          -0.441466       -1.309053        0.927708        0.685578   \n",
      "4          -0.223889       -0.265570        0.313394        1.544722   \n",
      "5           0.354656       -0.857087        0.380575        0.243536   \n",
      "6           0.361836       -0.188719       -0.397872       -0.911832   \n",
      "7           0.144748        0.104229       -1.153360       -0.066333   \n",
      "8           0.945786       -0.401218        1.424871       -0.137347   \n",
      "9          -0.101167       -1.840454       -0.362148        1.172895   \n",
      "10         -0.512634        0.963858       -1.033929       -1.046835   \n",
      "11         -2.768762        0.530712        1.250805       -0.062746   \n",
      "12         -1.956298       -1.743195       -0.592627       -0.138772   \n",
      "13         -0.879624       -1.067003        0.677909        0.393867   \n",
      "14          1.186089       -0.961196       -0.272780        0.615733   \n",
      "15         -0.507434        0.569227        0.463626        1.377338   \n",
      "16         -0.380297       -0.424790        0.720836        1.180664   \n",
      "17         -0.349605       -1.020582       -0.709953        0.116002   \n",
      "18         -1.231630       -1.609661       -1.127168       -1.120998   \n",
      "19         -1.239782        1.218215        0.893373       -1.034352   \n",
      "20         -0.351138       -0.350795        0.790340        0.300838   \n",
      "21         -0.768568       -0.325178       -0.222203        0.296209   \n",
      "22         -1.178820        0.152833       -0.503478       -0.056390   \n",
      "23          1.848593        0.009531        1.224301       -3.053927   \n",
      "24         -1.057037        1.057538       -0.796391       -0.671678   \n",
      "25         -0.809194        1.171703       -0.957726       -1.929238   \n",
      "26         -0.969250       -0.600918       -0.279904       -1.073296   \n",
      "27          0.599015        1.066869        0.348081       -2.146107   \n",
      "28          1.363917       -1.394446       -0.951288       -0.845274   \n",
      "29          2.502336        0.824652        0.655116        0.104130   \n",
      "...              ...             ...             ...             ...   \n",
      "7970       -0.582547        0.778589       -0.977144        0.329684   \n",
      "7971        1.378252        0.654392       -1.762682        0.557241   \n",
      "7972        0.244839       -0.176607        0.727341       -0.226625   \n",
      "7973        0.903473       -0.704925        1.576982       -0.238575   \n",
      "7974       -0.148261        0.595846       -0.542470        0.992079   \n",
      "7975        1.231429        1.033242       -1.832080       -0.154183   \n",
      "7976       -0.876554        0.123045       -0.360598       -0.611365   \n",
      "7977       -1.012148        0.642596        0.766711       -1.778143   \n",
      "7978       -0.047180       -1.631799        0.231401       -0.809285   \n",
      "7979       -0.376730       -0.811547       -1.418133        1.097097   \n",
      "7980        1.403287        0.354430       -1.753756       -2.060247   \n",
      "7981       -1.481141       -0.983009        0.834723        0.266428   \n",
      "7982       -0.566827       -1.030272        1.758796       -1.266479   \n",
      "7983       -0.360559        1.363758       -0.084157       -0.593799   \n",
      "7984       -0.139141       -1.087699       -0.195244       -0.914541   \n",
      "7985        2.256353        1.203730       -1.195529       -1.366207   \n",
      "7986        0.337549       -1.292725       -1.069068       -0.934035   \n",
      "7987       -0.438422       -1.835879        0.891317       -0.647243   \n",
      "7988       -0.957038       -0.641311        0.950329        0.202748   \n",
      "7989        0.355001        0.003138        0.819718       -1.291241   \n",
      "7990        0.461354       -0.113077        0.280262        0.164848   \n",
      "7991        0.196993       -0.190648       -0.036179       -0.267410   \n",
      "7992       -1.086166        0.403861        0.372280        0.829560   \n",
      "7993       -0.403821       -1.147086       -2.055694       -0.238909   \n",
      "7994        1.672198        0.976944        0.033080       -0.731114   \n",
      "7995       -1.067735       -0.665435        0.734198        1.331524   \n",
      "7996       -0.266511       -1.028978        2.729246       -1.771498   \n",
      "7997        2.278975        0.531648        0.742986       -2.770249   \n",
      "7998       -0.409141       -1.263315        0.078953        0.055088   \n",
      "7999        1.095281       -0.831555        0.167871       -0.205855   \n",
      "\n",
      "      feature_cont_6  feature_cont_7  ...  feature_cont_91  feature_cont_92  \\\n",
      "0           0.871859       -0.054092  ...        -0.756596         1.425213   \n",
      "1           2.391413        0.511989  ...        -1.145309         1.620197   \n",
      "2          -0.766974        0.086295  ...         1.643375         0.299507   \n",
      "3          -0.254723        0.009156  ...         0.031375        -0.054937   \n",
      "4           0.395023        0.048900  ...        -0.024253         0.643171   \n",
      "5          -0.300107        0.185597  ...         0.509729        -0.669613   \n",
      "6          -2.329013       -2.711318  ...        -1.213154        -0.227072   \n",
      "7           1.256729       -0.871837  ...        -0.723594         0.140541   \n",
      "8          -1.585813        0.295512  ...         1.162825        -0.349476   \n",
      "9          -0.029070       -0.333872  ...         0.323927        -0.101147   \n",
      "10         -0.093377        0.726981  ...        -0.051084        -0.166425   \n",
      "11          0.828587        1.660758  ...        -2.190689        -0.734966   \n",
      "12         -0.538145        0.587506  ...         1.429167        -0.265385   \n",
      "13          0.699757        1.190293  ...        -0.562555        -0.482529   \n",
      "14         -1.027349        0.736189  ...        -0.811646         0.154491   \n",
      "15         -1.055022        0.565076  ...         0.242330        -0.151632   \n",
      "16          0.232319       -2.326420  ...         0.155923        -1.298711   \n",
      "17         -0.286195        0.328008  ...         1.132683         0.402035   \n",
      "18          1.737432       -0.158336  ...         0.823924        -0.181722   \n",
      "19          0.796231        0.805404  ...        -0.010059        -1.950212   \n",
      "20         -0.877516        0.026544  ...         0.933868         1.156876   \n",
      "21          1.701156        2.539537  ...        -0.515699        -2.017572   \n",
      "22         -0.446139       -1.351484  ...         0.946035        -2.089542   \n",
      "23          1.515319        0.206402  ...        -0.674662         1.258297   \n",
      "24         -0.259656        0.374878  ...        -1.165732         0.890309   \n",
      "25          0.442513        0.828901  ...        -0.277102        -0.783994   \n",
      "26          0.542197        0.168552  ...         2.142909        -0.305092   \n",
      "27         -0.873990        0.885449  ...         0.346200         1.028686   \n",
      "28          0.120017       -0.432660  ...         0.707106        -1.428321   \n",
      "29         -0.232909        1.176311  ...        -1.602275        -0.367643   \n",
      "...              ...             ...  ...              ...              ...   \n",
      "7970       -0.023825       -0.689990  ...        -0.261558        -1.012312   \n",
      "7971        1.219832       -0.163080  ...         0.836074         0.546667   \n",
      "7972        0.014538        1.116269  ...        -0.031545        -1.033791   \n",
      "7973       -0.186344       -0.476661  ...         0.983315         0.024282   \n",
      "7974       -0.974682        1.833071  ...         0.463636         0.608064   \n",
      "7975        0.293041        0.490002  ...        -0.386930        -0.607936   \n",
      "7976       -0.060732       -0.648196  ...         0.189743         1.189533   \n",
      "7977       -0.796695        0.298870  ...         1.721000         0.466300   \n",
      "7978        1.447901        0.407075  ...        -0.752815        -0.044271   \n",
      "7979       -3.304389        0.724821  ...        -1.402180        -0.978750   \n",
      "7980        0.312098        1.993682  ...        -0.146618        -0.955742   \n",
      "7981        0.722798        1.513184  ...         0.470174         0.515169   \n",
      "7982        1.105634        0.281293  ...        -0.737954         2.104108   \n",
      "7983       -0.355326       -0.398536  ...        -1.846549        -0.030521   \n",
      "7984       -0.287458        0.659846  ...         0.593465        -0.097443   \n",
      "7985       -0.179882        0.283120  ...         0.066051         3.028712   \n",
      "7986        1.762834        0.166166  ...         0.526784        -2.035575   \n",
      "7987        2.284320       -1.272086  ...         0.541598         0.565621   \n",
      "7988        0.333293       -0.440677  ...         0.922396        -0.585287   \n",
      "7989        0.626013       -1.609481  ...        -0.335419         0.222416   \n",
      "7990       -2.790660       -2.009809  ...         0.992687         3.090517   \n",
      "7991        0.356225        1.804906  ...        -0.399863        -0.246003   \n",
      "7992        0.447120       -0.588714  ...        -0.524253        -0.791697   \n",
      "7993        0.847704        0.406580  ...        -0.940797        -1.384204   \n",
      "7994       -0.462673       -0.594641  ...         0.860919        -0.068700   \n",
      "7995        0.320947        1.038655  ...        -0.501486         2.429943   \n",
      "7996       -2.153399       -0.839848  ...        -0.411478        -0.189563   \n",
      "7997        0.468676        0.637778  ...        -0.440470        -0.605914   \n",
      "7998       -1.047225        0.530211  ...        -0.225936        -2.794048   \n",
      "7999       -0.828371       -0.148375  ...         0.927557        -0.592002   \n",
      "\n",
      "      feature_cont_93  feature_cont_94  feature_cont_95  feature_cont_96  \\\n",
      "0           -1.575258        -0.293165        -0.275897        -1.377679   \n",
      "1           -0.727287        -1.940734         0.050949         0.801726   \n",
      "2            0.088956         0.693218        -1.085919         0.467498   \n",
      "3            1.351858        -1.030064        -0.437976         0.142085   \n",
      "4            1.231148         0.499152        -0.681833         0.903398   \n",
      "5           -1.258152         1.391218         0.338304         1.246652   \n",
      "6            0.102337        -0.332875         1.299530        -0.757191   \n",
      "7            1.173886         0.464839         0.832801         0.989223   \n",
      "8           -1.491706        -1.124501         1.239251         0.318756   \n",
      "9           -0.228384         0.729505        -2.295036         2.043709   \n",
      "10           0.018946         0.206048         0.856914         1.515875   \n",
      "11           0.463847        -0.988955         2.229957        -0.580412   \n",
      "12           1.509977         0.187285        -0.520589        -0.516787   \n",
      "13           0.431134        -1.877976         1.650181         1.080791   \n",
      "14           1.869494         0.674835        -0.434912        -1.159943   \n",
      "15          -0.325571        -1.285106         1.715203        -1.125898   \n",
      "16          -0.718668         0.210700         0.667049        -1.302073   \n",
      "17          -0.182304        -0.663876         1.016913         0.468482   \n",
      "18          -1.182592        -1.206093        -1.360094         0.725650   \n",
      "19          -0.215953        -0.544753        -0.541954        -1.310187   \n",
      "20           0.544423        -0.619674         0.350076        -1.226192   \n",
      "21          -0.686578         0.863649         0.518463        -0.718766   \n",
      "22           0.049151        -1.064552        -0.081241         0.570959   \n",
      "23          -1.204747         0.515374         0.287211        -0.457063   \n",
      "24          -0.723582        -1.432276         0.272998        -2.356465   \n",
      "25           0.342209        -1.151243        -0.491001        -2.008997   \n",
      "26          -1.039411         1.702276        -1.883361        -0.421499   \n",
      "27           0.049515        -4.070224         0.227690         1.013277   \n",
      "28           1.406658         1.043031        -1.435992         1.205493   \n",
      "29          -0.756305        -0.902832         0.797691        -0.458460   \n",
      "...               ...              ...              ...              ...   \n",
      "7970         0.117304        -0.016239        -1.480399        -1.543713   \n",
      "7971        -0.454180         0.425247         0.602313        -2.178025   \n",
      "7972         1.439325        -0.138621        -0.048296        -1.236984   \n",
      "7973         0.990414        -0.723275         0.589766        -0.947755   \n",
      "7974        -0.087446         0.159935        -1.813406         1.453348   \n",
      "7975        -0.171112        -0.057205        -0.252637        -0.951275   \n",
      "7976         0.381793         0.754470         0.114060         0.839889   \n",
      "7977        -1.457825        -1.034801         1.267827        -0.142021   \n",
      "7978        -0.744965         0.365932        -0.520634         0.004310   \n",
      "7979         1.423856        -0.872209         0.805083         0.788100   \n",
      "7980        -1.006048        -1.685396         0.652019        -0.780773   \n",
      "7981         0.511341         0.622548         0.845691         0.129681   \n",
      "7982         0.675559         0.266291         0.411816         0.738433   \n",
      "7983         0.831491        -0.642521         0.097621        -0.705137   \n",
      "7984        -0.099949        -0.035625         1.042313         0.397121   \n",
      "7985        -0.937675         1.331106         0.537475        -1.125048   \n",
      "7986         1.091187         0.517655         0.604360        -0.438971   \n",
      "7987         0.057976        -0.003708        -0.365914        -0.090811   \n",
      "7988         0.156743        -0.751765         1.718822        -0.935228   \n",
      "7989        -0.284767         1.577161        -0.167946        -1.793681   \n",
      "7990         1.050713         0.732806         0.442601         1.492673   \n",
      "7991         1.230466         0.339091        -0.534608        -0.788522   \n",
      "7992        -0.064975         0.330877         0.204092         1.412258   \n",
      "7993        -1.734446         0.014212         1.672582         0.142283   \n",
      "7994         0.401648         1.053733         0.581971        -0.539773   \n",
      "7995         0.356462         0.386629         1.848574         0.024900   \n",
      "7996         0.043253         0.691171         0.225536         0.730557   \n",
      "7997         0.181726         0.126393         1.056606        -0.327251   \n",
      "7998        -0.838870         2.062506        -0.728815         0.145674   \n",
      "7999        -0.494005         1.114185         0.632933         0.601630   \n",
      "\n",
      "      feature_cont_97  feature_cont_98  feature_cont_99  target  \n",
      "0            2.075179        -0.592916         1.704407     1.0  \n",
      "1           -1.018189        -0.224059        -0.025025     0.0  \n",
      "2            0.329938         0.080603        -0.106436     0.0  \n",
      "3            0.435335        -1.885888         0.988952     1.0  \n",
      "4            0.130279        -0.318925        -0.092488     0.0  \n",
      "5            2.627643        -0.006989         2.625822     1.0  \n",
      "6            1.211154         0.898019         0.585113     0.0  \n",
      "7            0.514451         1.869821        -0.118199     0.0  \n",
      "8           -0.315326        -0.184357         0.637779     1.0  \n",
      "9           -1.549775         0.943635        -0.392981     1.0  \n",
      "10           0.391007         0.186296         1.587823     0.0  \n",
      "11           0.800571         0.368102         0.856392     1.0  \n",
      "12          -0.966165        -1.005282        -0.355517     1.0  \n",
      "13          -0.379512        -1.600899        -0.447581     1.0  \n",
      "14          -0.675225         0.470054         1.390602     1.0  \n",
      "15          -0.447975         0.491871         0.697289     1.0  \n",
      "16           2.011882        -1.187754         1.886325     0.0  \n",
      "17           0.226641        -0.265824         0.144878     1.0  \n",
      "18          -0.612781         1.503837        -0.413289     1.0  \n",
      "19          -0.092552        -0.524449        -0.861131     1.0  \n",
      "20           0.833487        -1.331033        -1.198430     0.0  \n",
      "21          -0.462142         1.753259         1.069409     0.0  \n",
      "22          -0.390594         1.718003        -1.569531     0.0  \n",
      "23          -0.079946        -1.064218        -0.099497     0.0  \n",
      "24           0.597212         0.900141        -0.025506     0.0  \n",
      "25           0.661993         0.191429        -1.315212     0.0  \n",
      "26          -1.608383        -0.817650        -1.664021     0.0  \n",
      "27           0.675109        -0.096625         1.074409     0.0  \n",
      "28           1.608621        -1.432746        -0.953391     1.0  \n",
      "29          -0.740360         0.608641        -1.758340     0.0  \n",
      "...               ...              ...              ...     ...  \n",
      "7970         0.717663         1.513640        -0.867820     1.0  \n",
      "7971         0.457509        -0.009857         1.595001     0.0  \n",
      "7972         0.608533         1.388012        -0.434492     1.0  \n",
      "7973         0.853261         0.596222        -0.154045     1.0  \n",
      "7974        -0.868893         1.044604        -1.771644     1.0  \n",
      "7975        -0.313295        -0.034721        -0.267716     0.0  \n",
      "7976        -0.712771        -0.685788        -1.725730     0.0  \n",
      "7977        -0.609081         0.581046         0.254957     1.0  \n",
      "7978         0.719155         0.293637        -0.822698     1.0  \n",
      "7979         0.571839        -1.118929        -0.859577     1.0  \n",
      "7980        -1.683252         0.729326        -0.651135     0.0  \n",
      "7981         0.732745         2.164379        -0.634942     1.0  \n",
      "7982        -2.394240         0.855389        -0.786897     1.0  \n",
      "7983        -0.171276        -0.378535         0.610722     0.0  \n",
      "7984        -0.033136        -1.783050         0.662757     1.0  \n",
      "7985        -1.472987        -0.160495        -0.311262     0.0  \n",
      "7986        -0.060980         1.186603        -0.055459     1.0  \n",
      "7987        -1.132071        -0.191315        -0.202938     1.0  \n",
      "7988        -1.925915         0.339507         0.484932     1.0  \n",
      "7989         1.735788        -0.714518         0.176232     1.0  \n",
      "7990        -0.211297        -0.399896        -0.362585     0.0  \n",
      "7991        -1.302000         1.730654         2.197332     0.0  \n",
      "7992         0.274858         0.734454         0.425213     1.0  \n",
      "7993         1.420947         0.336863         1.100940     0.0  \n",
      "7994         2.499890         1.019867         1.806574     0.0  \n",
      "7995        -0.655250        -1.027931         0.638042     0.0  \n",
      "7996        -0.917239         1.199784        -0.915459     1.0  \n",
      "7997        -1.214542         2.801443        -1.517449     1.0  \n",
      "7998        -0.458485        -0.099431        -1.443181     1.0  \n",
      "7999         0.803660        -1.966774         0.126424     1.0  \n",
      "\n",
      "[8000 rows x 103 columns]\n",
      "Index(['feature_cat_0', 'feature_cat_1', 'feature_cont_0', 'feature_cont_1',\n",
      "       'feature_cont_2', 'feature_cont_3', 'feature_cont_4', 'feature_cont_5',\n",
      "       'feature_cont_6', 'feature_cont_7',\n",
      "       ...\n",
      "       'feature_cont_91', 'feature_cont_92', 'feature_cont_93',\n",
      "       'feature_cont_94', 'feature_cont_95', 'feature_cont_96',\n",
      "       'feature_cont_97', 'feature_cont_98', 'feature_cont_99', 'target'],\n",
      "      dtype='object', length=103)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "filename = 'train_dataset.csv'\n",
    "train_gdf = cudf.io.csv.read_csv(filename)\n",
    "\n",
    "filename = 'test_dataset.csv'\n",
    "test_gdf = cudf.io.csv.read_csv(filename)\n",
    "# gdf = cudf.io.csv.read_csv(filename, index_col='Unnamed: 0')\n",
    "\n",
    "# filename = 'dataset.parquet'\n",
    "# num_rows, num_row_groups, names = cudf.io.parquet.read_parquet_metadata(filename)\n",
    "# gdf = [cudf.read_parquet(fname, row_group=i) for i in range(row_groups)]\n",
    "# gdf = cudf.concat(gdf)\n",
    "\n",
    "print(train_gdf.shape)\n",
    "print(train_gdf)\n",
    "print(train_gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset settings\n",
    "n_samples, n_cat_features, n_cont_features, n_classes = 1000, 2, 100, 2\n",
    "\n",
    "# create column names and instantiate preprocessor\n",
    "cat_names = ['feature_cat_{}'.format(i) for i in range(n_cat_features)]\n",
    "cont_names = ['feature_cont_{}'.format(i) for i in range(n_cont_features)]\n",
    "label_name = 'target'\n",
    "preprocessor = PreprocessDF(cat_names, cont_names, label_name, fill_strategy='median', to_cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/rapids/lib/python3.6/site-packages/cudf/io/dlpack.py:83: UserWarning: WARNING: cuDF to_dlpack() produces column-major (Fortran order) output. If the output tensor needs to be row major, transpose the output of this function.\n",
      "  return cpp_dlpack.to_dlpack(gdf_cols)\n"
     ]
    }
   ],
   "source": [
    "# preprocess data\n",
    "(X_cat_train, X_cont_train), y_train = preprocessor.preproc_dataframe(train_gdf, mode='train')\n",
    "(X_cat_test, X_cont_test), y_test = preprocessor.preproc_dataframe(train_gdf, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_train = X_cat_train.type(torch.FloatTensor)\n",
    "X_cont_train = X_cont_train.type(torch.FloatTensor)\n",
    "y_train = y_train.type(torch.LongTensor)\n",
    "X_cat_test = X_cat_test.type(torch.FloatTensor)\n",
    "X_cont_test = X_cont_test.type(torch.FloatTensor)\n",
    "y_test = y_test.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32 torch.float32 torch.int64\n",
      "torch.float32 torch.float32 torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(X_cat_train.dtype, X_cont_train.dtype, y_train.dtype)\n",
    "print(X_cat_test.dtype, X_cont_test.dtype, y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch datasets\n",
    "batch_size = 100\n",
    "train_dataset = TensorBatchDataset([X_cat_train, X_cont_train, y_train], \n",
    "                                   batch_size=batch_size, pin_memory=False)\n",
    "test_dataset = TensorBatchDataset([X_cat_test, X_cont_test, y_test], \n",
    "                                  batch_size=batch_size, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 102 2\n"
     ]
    }
   ],
   "source": [
    "# model training settings\n",
    "epochs = int(5)\n",
    "n_inputs = int(X_cat_train.size(1) + X_cont_train.size(1))\n",
    "n_outputs = int(2)\n",
    "learning_rate = 0.001\n",
    "\n",
    "print(epochs, n_inputs, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch data loaders\n",
    "train_data_loader = BatchDataLoader(train_dataset, shuffle=False,\n",
    "                                    pin_memory=False, drop_last=False, device='cuda')\n",
    "test_data_loader = BatchDataLoader(test_dataset, shuffle=False,\n",
    "                                   pin_memory=False, drop_last=False, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(\n",
      "  (linear): Linear(in_features=102, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiate model, loss, and optimizer\n",
    "model = LogisticRegression(n_inputs, n_outputs)\n",
    "model = model.cuda()\n",
    "print(model)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1. Batch Number: 100. Loss: 0.007068986892700195. Accuracy: 55.\n",
      "Epoch: 2. Batch Number: 200. Loss: 0.006630552291870117. Accuracy: 60.\n",
      "Epoch: 3. Batch Number: 300. Loss: 0.0062426824569702145. Accuracy: 65.\n",
      "Epoch: 4. Batch Number: 400. Loss: 0.005894451141357422. Accuracy: 69.\n"
     ]
    }
   ],
   "source": [
    "# train the model by feeding in batches of data\n",
    "batch_number = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    for i, batch in enumerate(train_data_loader):\n",
    "        # unpack batch\n",
    "        (X_cat_batch, X_cont_batch), y_batch = batch\n",
    "        X_batch = torch.cat((X_cat_batch, X_cont_batch), 1)\n",
    "        \n",
    "        # create variables from inputs and outputs\n",
    "        X_batch = Variable(X_batch)\n",
    "        y_batch = Variable(y_batch)\n",
    "        \n",
    "        # zero out gradients and use model to create outputs\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "        # calculate loss and backpropogate\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # every 100 batches, evaluate on test dataset\n",
    "        batch_number += 1\n",
    "        if batch_number % 100 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            total_loss = 0\n",
    "            for batch in test_data_loader:\n",
    "                # unpack batch\n",
    "                (X_cat_batch, X_cont_batch), y_batch = batch\n",
    "                X_batch = torch.cat((X_cat_batch, X_cont_batch), 1)\n",
    "\n",
    "                # create variables from inputs and outputs\n",
    "                X_batch = Variable(X_batch)\n",
    "                Y_batch = Variable(y_batch)\n",
    "                \n",
    "                # use model to create outputs\n",
    "                outputs = model(X_batch)\n",
    "                \n",
    "                # calculate loss\n",
    "                test_loss = criterion(outputs, y_batch)\n",
    "                total_loss += test_loss\n",
    "                \n",
    "                # calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum()\n",
    "            accuracy = 100 * correct / total\n",
    "            print(\"Epoch: {}. Batch Number: {}. Loss: {}. Accuracy: {}.\".format(epoch, batch_number, total_loss.item() / total, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
