{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from batchloader import TensorBatchDataset, BatchDataLoader\n",
    "from preprocess import PreprocessDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "filename = 'train_dataset.csv'\n",
    "train_gdf = cudf.io.csv.read_csv(filename)\n",
    "\n",
    "filename = 'test_dataset.csv'\n",
    "test_gdf = cudf.io.csv.read_csv(filename)\n",
    "# gdf = cudf.io.csv.read_csv(filename, index_col='Unnamed: 0')\n",
    "\n",
    "# filename = 'dataset.parquet'\n",
    "# num_rows, num_row_groups, names = cudf.io.parquet.read_parquet_metadata(filename)\n",
    "# gdf = [cudf.read_parquet(fname, row_group=i) for i in range(row_groups)]\n",
    "# gdf = cudf.concat(gdf)\n",
    "\n",
    "print(train_gdf.shape)\n",
    "print(train_gdf)\n",
    "print(train_gdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset settings\n",
    "n_samples, n_cat_features, n_cont_features, n_classes = 1000, 2, 100, 2\n",
    "\n",
    "# create column names and instantiate preprocessor\n",
    "cat_names = ['feature_cat_{}'.format(i) for i in range(n_cat_features)]\n",
    "cont_names = ['feature_cont_{}'.format(i) for i in range(n_cont_features)]\n",
    "label_name = 'target'\n",
    "preprocessor = PreprocessDF(cat_names, cont_names, label_name, fill_strategy='median', to_cpu=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess data\n",
    "(X_cat_train, X_cont_train), y_train = preprocessor.preproc_dataframe(train_gdf, mode='train')\n",
    "(X_cat_test, X_cont_test), y_test = preprocessor.preproc_dataframe(train_gdf, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cat_train = X_cat_train.type(torch.FloatTensor)\n",
    "X_cont_train = X_cont_train.type(torch.FloatTensor)\n",
    "y_train = y_train.type(torch.LongTensor)\n",
    "X_cat_test = X_cat_test.type(torch.FloatTensor)\n",
    "X_cont_test = X_cont_test.type(torch.FloatTensor)\n",
    "y_test = y_test.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_cat_train.dtype, X_cont_train.dtype, y_train.dtype)\n",
    "print(X_cat_test.dtype, X_cont_test.dtype, y_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch datasets\n",
    "batch_size = 100\n",
    "train_dataset = TensorBatchDataset([X_cat_train, X_cont_train, y_train], \n",
    "                                   batch_size=batch_size, pin_memory=False)\n",
    "test_dataset = TensorBatchDataset([X_cat_test, X_cont_test, y_test], \n",
    "                                  batch_size=batch_size, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training settings\n",
    "epochs = int(5)\n",
    "n_inputs = int(X_cat_train.size(1) + X_cont_train.size(1))\n",
    "n_outputs = int(2)\n",
    "learning_rate = 0.001\n",
    "\n",
    "print(epochs, n_inputs, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create batch data loaders\n",
    "train_data_loader = BatchDataLoader(train_dataset, shuffle=False,\n",
    "                                    pin_memory=False, drop_last=False, device='cuda')\n",
    "test_data_loader = BatchDataLoader(test_dataset, shuffle=False,\n",
    "                                   pin_memory=False, drop_last=False, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(n_inputs, n_outputs)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.linear(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model, loss, and optimizer\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "model = model.cuda()\n",
    "print(model)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model by feeding in batches of data\n",
    "batch_number = 0\n",
    "for epoch in range(int(epochs)):\n",
    "    for i, batch in enumerate(train_data_loader):\n",
    "        # unpack batch\n",
    "        (X_cat_batch, X_cont_batch), y_batch = batch\n",
    "        X_batch = torch.cat((X_cat_batch, X_cont_batch), 1)\n",
    "        \n",
    "        # create variables from inputs and outputs\n",
    "        X_batch = Variable(X_batch)\n",
    "        y_batch = Variable(y_batch)\n",
    "        \n",
    "        # zero out gradients and use model to create outputs\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "        # calculate loss and backpropogate\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # every 100 batches, evaluate on test dataset\n",
    "        batch_number += 1\n",
    "        if batch_number % 100 == 0:\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            total_loss = 0\n",
    "            for batch in test_data_loader:\n",
    "                # unpack batch\n",
    "                (X_cat_batch, X_cont_batch), y_batch = batch\n",
    "                X_batch = torch.cat((X_cat_batch, X_cont_batch), 1)\n",
    "\n",
    "                # create variables from inputs and outputs\n",
    "                X_batch = Variable(X_batch)\n",
    "                Y_batch = Variable(y_batch)\n",
    "                \n",
    "                # use model to create outputs\n",
    "                outputs = model(X_batch)\n",
    "                \n",
    "                # calculate loss\n",
    "                test_loss = criterion(outputs, y_batch)\n",
    "                total_loss += test_loss\n",
    "                \n",
    "                # calculate accuracy\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += y_batch.size(0)\n",
    "                correct += (predicted == y_batch).sum()\n",
    "            accuracy = 100 * correct / total\n",
    "            print(\"Epoch: {}. Batch Number: {}. Loss: {}. Accuracy: {}.\".format(epoch, batch_number, total_loss.item() / total, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
